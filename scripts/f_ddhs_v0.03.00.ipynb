{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.datasets.demo import download_demo\n",
    "real_data, metadata = download_demo(\n",
    "    modality='single_table',\n",
    "    dataset_name='fake_hotel_guests')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.lite import SingleTablePreset\n",
    "\n",
    "synthesizer = SingleTablePreset(metadata, name='FAST_ML')\n",
    "synthesizer.fit(data=real_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from sdv.single_table import CTGANSynthesizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sdv.metadata import SingleTableMetadata\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sdv.lite import SingleTablePreset\n",
    "\n",
    "# push + prd\n",
    "\n",
    "# imbalanced에 data level로 해결하는 모델\n",
    "class FiGen:\n",
    "    def __init__(self, ratio: float, index: List[str]):\n",
    "        \"\"\"\n",
    "        고정적으로 사용하는 값을 저장\n",
    "        \n",
    "        Args:\n",
    "            ratio (float): small class+생성된 데이터와 large class의 비율 \n",
    "            index (List[int]): 범주형, 연속형 구분하기 위한 연속형 변수의 컬럼명 인덱스       \n",
    "        \"\"\"\n",
    "        self.result = 0\n",
    "        self.ratio = ratio\n",
    "        self.index = index\n",
    "    \n",
    "\n",
    "    def extract_middle_percent(self, data: pd.DataFrame, start: float, last:float):\n",
    "        \"\"\"\n",
    "        데이터의 분포 중 중간 부분을 추출 \n",
    "        \n",
    "        Args:\n",
    "            data : 입력 데이터\n",
    "            start : 추출 시작 percentile \n",
    "            last : 추출 끝 percentile\n",
    "        Returns:    \n",
    "            데이터의 분포 중 중간 부분을 추출하여 리턴\n",
    "        \"\"\"\n",
    "        scaler = StandardScaler()\n",
    "        data_scaled = scaler.fit_transform(data.values)\n",
    "        kde = KernelDensity(kernel=\"gaussian\", bandwidth=0.5).fit(\n",
    "            data_scaled\n",
    "        )  ##TODO: 계산이 안터지도록 하기, gmm으로 변경\n",
    "        log_prob = kde.score_samples(data_scaled)\n",
    "        prob = np.exp(log_prob)\n",
    "        threshold_low, threshold_high = np.percentile(prob, [start, last])\n",
    "        mask = np.logical_and(prob >= threshold_low, prob <= threshold_high)\n",
    "        data_middle = data[mask]\n",
    "\n",
    "        if len(data_middle) > 0:\n",
    "            return data_middle\n",
    "        else:\n",
    "            print(\"No middle 50% found, returning original data\")\n",
    "            return []\n",
    "        \n",
    "    def find_categorical(\n",
    "        self, suitable_generated_small_X: pd.DataFrame, categorical_small_X: pd.DataFrame, small_X: pd.DataFrame\n",
    "    ):  \n",
    "        \"\"\"\n",
    "        생성된 연속형변수와 기존 연속형 변수의 cosine simmilarity를 기준으로 가장 가까운 기존 변수를 찾은 후 해당 변수의 범주형 값을 가져옴\n",
    "        \n",
    "        Args:\n",
    "            suitable_generated_small_X : 생성된 적합한 small class의 연속형 변수만 있는 x \n",
    "            small_X : small class의 연속형, 범주형 변수가 모두 있는 orgin x\n",
    "        Returns:\n",
    "            생성된 연속 변수를 범주형 변수값이 결합된 형태로 리턴 \n",
    "        \"\"\"\n",
    "\n",
    "        # Min-Max 스케일링을 위한 객체 생성\n",
    "        scaler = MinMaxScaler()\n",
    "\n",
    "        # 열별 Min-Max 스케일링 수행\n",
    "        suitable_generated_small_scaled_X = pd.DataFrame(\n",
    "            scaler.fit_transform(suitable_generated_small_X),\n",
    "            columns=suitable_generated_small_X.columns,\n",
    "        )\n",
    " \n",
    "        orgin_small_non_cat_scaled_X = pd.DataFrame(\n",
    "            scaler.fit_transform(small_X[self.index]),\n",
    "            columns=self.index\n",
    "        )\n",
    "\n",
    "        # 데이터프레임을 numpy 배열로 변환\n",
    "        array_mxn = suitable_generated_small_scaled_X.values\n",
    "        array_kxn = orgin_small_non_cat_scaled_X.values\n",
    "    \n",
    "        # 코사인 유사도 계산\n",
    "        cosine_similarities = cosine_similarity(array_mxn, array_kxn)\n",
    "      \n",
    "        # 각 행에서 최대값을 가지는 열의 인덱스를 가져와서 리스트로 만들기\n",
    "        max_indices = np.argmax(cosine_similarities, axis=1).tolist()\n",
    "        \n",
    "\n",
    "        # 중복된 행 인덱스에 해당하는 데이터 선택하여 배열에 저장\n",
    "        combined_rows = np.concatenate([\n",
    "        suitable_generated_small_X,\n",
    "        categorical_small_X.values[max_indices]\n",
    "        ], axis=1)\n",
    "\n",
    "        # 모아진 행들을 데이터프레임으로 변환하여 synthetic_small_X 생성\n",
    "        column_names = (\n",
    "            suitable_generated_small_scaled_X.columns.tolist() +\n",
    "            categorical_small_X.columns.tolist()\n",
    "            )\n",
    "        synthetic_small_X = pd.DataFrame(combined_rows, columns=column_names)\n",
    "\n",
    "        return synthetic_small_X\n",
    "\n",
    "    def suitable_judge(self, midlle_small_X:pd.DataFrame, small_X: pd.DataFrame, large_X: pd.DataFrame):\n",
    "        \"\"\"\n",
    "           generated_x : 생성된 small class x 데이터\n",
    "           small_X : 원본 small class x 데이터 \n",
    "           large_X : 원본 large class x 데이터\n",
    "        \"\"\"\n",
    "        # 연속형small x로 뽑아야함\n",
    "        center_small_X = np.mean(\n",
    "            small_X[self.index].values, axis=0, dtype=np.float64, out=None \n",
    "        )\n",
    "        radius_small_X = np.max(\n",
    "            np.linalg.norm(small_X[self.index].values - center_small_X, axis=1)\n",
    "        )\n",
    "\n",
    "        center_large_X = np.mean(\n",
    "            large_X[self.index].values, axis=0, dtype=np.float64, out=None \n",
    "        )\n",
    "\n",
    "        radius_large_X = np.max(\n",
    "            np.linalg.norm(large_X[self.index].values - center_large_X, axis=1)\n",
    "        )\n",
    "\n",
    "        synthetic_sample = pd.DataFrame()  # 최종 합치기\n",
    "       \n",
    "\n",
    "        # ctgan으로 연속형 생성 부분\n",
    "        metadata = SingleTableMetadata()\n",
    "        metadata.detect_from_dataframe(data=midlle_small_X)\n",
    "        \n",
    "        synthesizer = SingleTablePreset(metadata, name='FAST_ML')\n",
    "        synthesizer.fit(data=midlle_small_X)\n",
    "        \n",
    "        \n",
    "        # 합성된 개수 / 원래 large 클래스 개수 <= ratio 만족시 그만 생성    \n",
    "        \n",
    "        while len(synthetic_sample) / len(large_X) < self.ratio:\n",
    "\n",
    "            # large class의 데이터 사이즈 10배 만큼 데이터 생성\n",
    "            synthetic_data = synthesizer.sample(num_rows=len(large_X))  \n",
    "\n",
    "            synthetic_samples_to_generate = int((self.ratio - len(synthetic_sample) / len(large_X)) * len(large_X))\n",
    "            if synthetic_samples_to_generate == 0:\n",
    "                break  # 더 이상 생성이 필요하지 않을 경우 루프를 빠져나감\n",
    "            z = synthetic_data.iloc[:synthetic_samples_to_generate]  # 벡터화된 방식으로 일괄 처리\n",
    "        \n",
    "            distances_small = np.linalg.norm(z.values[:, np.newaxis, :] - center_small_X, axis=2)\n",
    "            distances_large = np.linalg.norm(z.values[:, np.newaxis, :] - center_large_X, axis=2)\n",
    "        \n",
    "            small_condition = distances_small < radius_small_X\n",
    "            large_condition = distances_large < radius_large_X\n",
    "\n",
    "            # 생성된 small class 데이터가 small, large class 중 small에 가까운지, small class의 지름을 넘지는 않는지\n",
    "            condition = np.logical_and(small_condition, distances_small < distances_large)\n",
    "        \n",
    "            synthetic_sample = pd.concat([synthetic_sample, z[condition]])\n",
    "            \n",
    "        return synthetic_sample.reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    def generate_synthetic(\n",
    "        self, small_X: pd.DataFrame, large_X: pd.DataFrame, small_Y: pd.DataFrame, large_Y: pd.DataFrame\n",
    "    ) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "        \"\"\"\n",
    "        생성된 데이터셋 + 기존 데이터셋을 합쳐 통합 데이터셋을 생성\n",
    "        \n",
    "        Args:\n",
    "            small_X (pd.DataFrame): small class의 x\n",
    "            large_X (pd.DataFrame): large class의 x\n",
    "        Returns:\n",
    "            생성된 데이터셋 + 기존 데이터셋을 합쳐 통합 데이터셋을 리턴\n",
    "        \"\"\"\n",
    "\n",
    "        # Nan 값 제거 요청 \n",
    "        assert not large_X.isnull().values.any(), \"large_X 입력 데이터에 NaN 값이 포함되어 있습니다.\" \n",
    "        assert not small_X.isnull().values.any(), \"small_X 입력 데이터에 NaN 값이 포함되어 있습니다.\"    \n",
    " \n",
    "        # 연속형 변수만 가져오는 부분\n",
    "        continue_small_X = small_X[self.index]\n",
    "        continue_large_X = large_X[self.index]\n",
    "\n",
    "        # 범주형 변수만 가져오는 부분\n",
    "        categorical_small_X = small_X[list(set(small_X.columns) - set(self.index))]\n",
    "        categorical_large_X = large_X[list(set(small_X.columns) - set(self.index))]\n",
    "\n",
    "        # 상위 n% 필터링 부분\n",
    "        midlle_small_X = self.extract_middle_percent(\n",
    "            continue_small_X, 25, 75\n",
    "        )  ##TODO: 추후에 하이퍼 파라미터로 뺄 수 있음\n",
    "        midlle_large_X = self.extract_middle_percent(\n",
    "            continue_large_X, 15, 85\n",
    "        )  ##TODO: 추후에 하이퍼 파라미터로 뺄 수 있음\n",
    "        \n",
    "        # 연속형 데이터 생성 및 데이터 적합 판단\n",
    "        suitable_generated_small_X = self.suitable_judge(midlle_small_X, small_X, large_X)\n",
    "     \n",
    "        # 코사인 유사도 기반으로 가장 가까운 기존 변수의 범주형 변수 값 가져오기\n",
    "        synthetic_small_X = self.find_categorical(\n",
    "            suitable_generated_small_X, categorical_small_X, small_X \n",
    "        )\n",
    "\n",
    "        # small class와 large class 합치기\n",
    "        origin_small_x = pd.concat(\n",
    "            [midlle_small_X, categorical_small_X.loc[midlle_small_X.index]], axis=1\n",
    "        )\n",
    "        \n",
    "        small_total_x = pd.concat([synthetic_small_X, origin_small_x], axis=0)\n",
    "        \n",
    "        small_total_x[\"target\"] = small_Y.iloc[:1].values[0][0]\n",
    "\n",
    "        origin_large_x = pd.concat(\n",
    "            [midlle_large_X, categorical_large_X.loc[midlle_large_X.index]], axis=1\n",
    "        )\n",
    "    \n",
    "        origin_large_x[\"target\"] = large_Y.iloc[:1].values[0][0]\n",
    "  \n",
    "        total = pd.concat([small_total_x, origin_large_x], axis=0)\n",
    "\n",
    "        return total.drop(columns=[\"target\"]), total[\"target\"]\n",
    "    \n",
    "    \n",
    "    def fit(\n",
    "        self,\n",
    "        small_X: pd.DataFrame,\n",
    "        small_Y: pd.DataFrame,\n",
    "        large_X: pd.DataFrame,\n",
    "        large_Y: pd.DataFrame        \n",
    "    ):\n",
    "        \"\"\"\n",
    "        데이터를 학습 시키는 함수\n",
    "        Args:\n",
    "            small_X (pd.DataFrame): small class의 x\n",
    "            small_Y (pd.DataFrame): small class의 y\n",
    "            large_X (pd.DataFrame): large class의 x\n",
    "            large_Y (pd.DataFrame): large class의 y\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.DataFrame]: synthetic X, y\n",
    "        \n",
    "        \"\"\"\n",
    "        # 합성+ 기존 data set 생성\n",
    "        synthetic_X, synthetic_Y = self.generate_synthetic(\n",
    "            small_X, large_X, small_Y, large_Y\n",
    "        )\n",
    "        return synthetic_X, synthetic_Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEN = FiGen(0.3,['amenities_fee','room_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = has_rewards\n",
    "real_data = real_data.dropna(axis=0)\n",
    "small_X = real_data[real_data['has_rewards'] == True]\n",
    "small_Y = real_data[real_data['has_rewards'] == True].iloc[:, [1]]\n",
    "large_X = real_data[real_data['has_rewards'] == False]\n",
    "large_Y = real_data[real_data['has_rewards'] == False].iloc[:, [1]]\n",
    "\n",
    "# 생성된 것 중에 적합헌 것이 없어 넘어가는 과장이 원활히 되지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_X, synthetic_Y = GEN.fit(small_X, small_Y, large_X, large_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_index = ['amenities_fee','room_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = large_X[con_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_kernel(data):\n",
    "    # 추출할 분위수 범위 설정\n",
    "    a_percentile = 25\n",
    "    b_percentile = 75\n",
    "\n",
    "    # 각 열의 분위수 값 계산\n",
    "    percentiles = np.percentile(data, [a_percentile, b_percentile], axis=0)\n",
    "\n",
    "    # 각 열별로 a < x < b 범위에 해당하는 데이터 추출\n",
    "    condition = np.all((data > percentiles[0, :]) & (data < percentiles[1, :]), axis=1)\n",
    "    \n",
    "    return condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "kde = KernelDensity(kernel=\"gaussian\", bandwidth=0.5).fit(\n",
    "            data_scaled\n",
    "        ) \n",
    "log_prob = kde.score_samples(data_scaled)\n",
    "prob = np.exp(log_prob)\n",
    "threshold_low, threshold_high = np.percentile(prob, [25, 75])\n",
    "mask = np.logical_and(prob >= threshold_low, prob <= threshold_high)\n",
    "data_middle = data[mask]\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "self_kde = extract_kernel(data_scaled)\n",
    "data_middle2= data[self_kde]\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fastkde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import fastkde\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# 분위수 범위 설정\n",
    "a = 0.25\n",
    "b = 0.75\n",
    "\n",
    "# fastKDE를 위해 데이터를 전치하여 사용\n",
    "data_transposed = data_scaled.T\n",
    "\n",
    "# fastKDE로 다변량 KDE 계산\n",
    "kde = fastKDE.pdf(*data_transposed)\n",
    "\n",
    "# 분위수 범위에 해당하는 데이터 추출\n",
    "grid = kde[0]  # KDE의 grid\n",
    "selected_indices = np.where((grid >= a) & (grid <= b))[0]\n",
    "\n",
    "# 추출된 데이터 출력\n",
    "selected_data = data.iloc[selected_indices]\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(selected_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_middle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분위수 범위 설정\n",
    "a = 0.25\n",
    "b = 0.75\n",
    "\n",
    "# KDE 밀도 함수에서 a부터 b까지 해당하는 데이터 추출\n",
    "selected_indices = np.where((axes >= a) & (axes <= b))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
