{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.datasets.demo import download_demo\n",
    "real_data, metadata = download_demo(\n",
    "    modality='single_table',\n",
    "    dataset_name='fake_hotel_guests')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.lite import SingleTablePreset\n",
    "\n",
    "synthesizer = SingleTablePreset(metadata, name='FAST_ML')\n",
    "synthesizer.fit(data=real_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from sdv.single_table import CTGANSynthesizer\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sdv.metadata import SingleTableMetadata\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sdv.lite import SingleTablePreset\n",
    "\n",
    "# push + prd\n",
    "\n",
    "# imbalanced에 data level로 해결하는 모델\n",
    "class FiGen:\n",
    "    def __init__(self, ratio: float, index: List[str]):\n",
    "        \"\"\"\n",
    "        고정적으로 사용하는 값을 저장\n",
    "        \n",
    "        Args:\n",
    "            ratio (float): small class+생성된 데이터와 large class의 비율 \n",
    "            index (List[int]): 범주형, 연속형 구분하기 위한 연속형 변수의 컬럼명 인덱스       \n",
    "        \"\"\"\n",
    "        self.result = 0\n",
    "        self.ratio = ratio\n",
    "        self.index = index\n",
    "\n",
    "\n",
    "    def extract_middle_percent(self, data: pd.DataFrame, start: float, last:float):\n",
    "        \"\"\"\n",
    "        데이터의 분포 중 중간 부분을 추출 \n",
    "        \n",
    "        Args:\n",
    "            data : 입력 데이터\n",
    "            start : 추출 시작 percentile \n",
    "            last : 추출 끝 percentile\n",
    "        Returns:    \n",
    "            데이터의 분포 중 중간 부분을 추출하여 리턴\n",
    "        \"\"\"\n",
    "        scaler = StandardScaler()\n",
    "        data_scaled = scaler.fit_transform(data.values)\n",
    "        kde = KernelDensity(kernel=\"gaussian\", bandwidth=0.5).fit(\n",
    "            data_scaled\n",
    "        )  ##TODO: 계산이 안터지도록 하기, gmm으로 변경\n",
    "        log_prob = kde.score_samples(data_scaled)\n",
    "        prob = np.exp(log_prob)\n",
    "        threshold_low, threshold_high = np.percentile(prob, [start, last])\n",
    "        mask = np.logical_and(prob >= threshold_low, prob <= threshold_high)\n",
    "        data_middle = data[mask]\n",
    "\n",
    "        if len(data_middle) > 0:\n",
    "            return data_middle\n",
    "        else:\n",
    "            print(\"No middle 50% found, returning original data\")\n",
    "            return []\n",
    "        \n",
    "    def find_categorical(\n",
    "        self, suitable_generated_small_X: pd.DataFrame, categorical_small_X: pd.DataFrame, small_X: pd.DataFrame\n",
    "    ):  \n",
    "        \"\"\"\n",
    "        생성된 연속형변수와 기존 연속형 변수의 cosine simmilarity를 기준으로 가장 가까운 기존 변수를 찾은 후 해당 변수의 범주형 값을 가져옴\n",
    "        \n",
    "        Args:\n",
    "            suitable_generated_small_X : 생성된 적합한 small class의 연속형 변수만 있는 x \n",
    "            small_X : small class의 연속형, 범주형 변수가 모두 있는 orgin x\n",
    "        Returns:\n",
    "            생성된 연속 변수를 범주형 변수값이 결합된 형태로 리턴 \n",
    "        \"\"\"\n",
    "\n",
    "        # Min-Max 스케일링을 위한 객체 생성\n",
    "        scaler = MinMaxScaler()\n",
    "\n",
    "\n",
    "        # 열별 Min-Max 스케일링 수행\n",
    "        suitable_generated_small_scaled_X = pd.DataFrame(\n",
    "            scaler.fit_transform(suitable_generated_small_X),\n",
    "            columns=suitable_generated_small_X.columns,\n",
    "        )\n",
    " \n",
    "        orgin_small_non_cat_scaled_X = pd.DataFrame(\n",
    "            scaler.fit_transform(small_X[self.index]),\n",
    "            columns=self.index\n",
    "        )\n",
    "\n",
    "        # 데이터프레임을 numpy 배열로 변환\n",
    "        array_mxn = suitable_generated_small_scaled_X.values\n",
    "        array_kxn = orgin_small_non_cat_scaled_X.values\n",
    "        \n",
    "        # 행렬곱 수행 (mxn과 nxk로 계산)\n",
    "        result_array = np.dot(array_mxn, array_kxn.T)\n",
    "\n",
    "        # 각 행에서 최대값을 가지는 열의 인덱스를 가져와서 리스트로 만들기\n",
    "        max_indices = np.argmax(result_array, axis=1).tolist()\n",
    "\n",
    "        # 가장큰 열 인덱스가 들어있는 리스트의 인덱스에 따라 범주형 값 가져오기\n",
    "        synthetic_small_X = pd.concat(\n",
    "            [suitable_generated_small_scaled_X, categorical_small_X.loc[max_indices]],axis=1\n",
    "        )\n",
    "\n",
    "        return synthetic_small_X\n",
    "\n",
    "    def suitable_judge(self, midlle_small_X:pd.DataFrame, small_X: pd.DataFrame, large_X: pd.DataFrame):\n",
    "        \"\"\"\n",
    "           generated_x : 생성된 small class x 데이터\n",
    "           small_X : 원본 small class x 데이터 \n",
    "           large_X : 원본 large class x 데이터\n",
    "        \"\"\"\n",
    "        # 연속형small x로 뽑아야함\n",
    "        center_small_X = np.mean(\n",
    "            small_X[self.index].values, axis=0, dtype=np.float64, out=None \n",
    "        )\n",
    "        radius_small_X = np.max(\n",
    "            np.linalg.norm(small_X[self.index].values - center_small_X, axis=1)\n",
    "        )\n",
    "\n",
    "        center_large_X = np.mean(\n",
    "            large_X[self.index].values, axis=0, dtype=np.float64, out=None \n",
    "        )\n",
    "\n",
    "        radius_large_X = np.max(\n",
    "            np.linalg.norm(large_X[self.index].values - center_large_X, axis=1)\n",
    "        )\n",
    "\n",
    "        synthetic_sample = pd.DataFrame()  # 최종 합치기\n",
    "       \n",
    "\n",
    "        # ctgan으로 연속형 생성 부분\n",
    "        metadata = SingleTableMetadata()\n",
    "        metadata.detect_from_dataframe(data=midlle_small_X)\n",
    "        \n",
    "        synthesizer = SingleTablePreset(metadata, name='FAST_ML')\n",
    "        synthesizer.fit(data=midlle_small_X)\n",
    "        \n",
    "        \n",
    "        # 합성된 개수 / 원래 large 클래스 개수 <= ratio 만족시 그만 생성    \n",
    "        \n",
    "        while len(synthetic_sample) / len(large_X) < self.ratio:\n",
    "\n",
    "            # large class의 데이터 사이즈 10배 만큼 데이터 생성\n",
    "            synthetic_data = synthesizer.sample(num_rows=len(large_X))  \n",
    "\n",
    "            synthetic_samples_to_generate = int((self.ratio - len(synthetic_sample) / len(large_X)) * len(large_X))\n",
    "            if synthetic_samples_to_generate == 0:\n",
    "                break  # 더 이상 생성이 필요하지 않을 경우 루프를 빠져나감\n",
    "            z = synthetic_data.iloc[:synthetic_samples_to_generate]  # 벡터화된 방식으로 일괄 처리\n",
    "        \n",
    "            distances_small = np.linalg.norm(z.values[:, np.newaxis, :] - center_small_X, axis=2)\n",
    "            distances_large = np.linalg.norm(z.values[:, np.newaxis, :] - center_large_X, axis=2)\n",
    "        \n",
    "            small_condition = distances_small < radius_small_X\n",
    "            large_condition = distances_large < radius_large_X\n",
    "\n",
    "            # 생성된 small class 데이터가 small, large class 중 small에 가까운지, small class의 지름을 넘지는 않는지\n",
    "            condition = np.logical_and(small_condition, distances_small < distances_large)\n",
    "        \n",
    "            synthetic_sample = pd.concat([synthetic_sample, z[condition]])\n",
    "            \n",
    "        return synthetic_sample.reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    def generate_synthetic(\n",
    "        self, small_X: pd.DataFrame, large_X: pd.DataFrame, small_Y: pd.DataFrame, large_Y: pd.DataFrame\n",
    "    ) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "        \"\"\"\n",
    "        생성된 데이터셋 + 기존 데이터셋을 합쳐 통합 데이터셋을 생성\n",
    "        \n",
    "        Args:\n",
    "            small_X (pd.DataFrame): small class의 x\n",
    "            large_X (pd.DataFrame): large class의 x\n",
    "        Returns:\n",
    "            생성된 데이터셋 + 기존 데이터셋을 합쳐 통합 데이터셋을 리턴\n",
    "        \"\"\"\n",
    "\n",
    "        # Nan 값 제거 요청 \n",
    "        assert not large_X.isnull().values.any(), \"large_X 입력 데이터에 NaN 값이 포함되어 있습니다.\" \n",
    "        assert not small_X.isnull().values.any(), \"small_X 입력 데이터에 NaN 값이 포함되어 있습니다.\"    \n",
    " \n",
    "\n",
    "        # 연속형 변수만 가져오는 부분\n",
    "        continue_small_X = small_X[self.index]\n",
    "        continue_large_X = large_X[self.index]\n",
    "\n",
    "        # 범주형 변수만 가져오는 부분\n",
    "        categorical_small_X = small_X[list(set(small_X.columns) - set(self.index))]\n",
    "        categorical_large_X = large_X[list(set(small_X.columns) - set(self.index))]\n",
    "\n",
    "    \n",
    "        # 상위 n% 필터링 부분\n",
    "        midlle_small_X = self.extract_middle_percent(\n",
    "            continue_small_X, 25, 75\n",
    "        )  ##TODO: 추후에 하이퍼 파라미터로 뺄 수 있음\n",
    "        midlle_large_X = self.extract_middle_percent(\n",
    "            continue_large_X, 15, 85\n",
    "        )  ##TODO: 추후에 하이퍼 파라미터로 뺄 수 있음\n",
    "        \n",
    "        # 연속형 데이터 생성 및 데이터 적합 판단\n",
    "\n",
    "        suitable_generated_small_X = self.suitable_judge(midlle_small_X, small_X, large_X)\n",
    "      \n",
    "        # 코사인 유사도 기반으로 가장 가까운 기존 변수의 범주형 변수 값 가져오기\n",
    "  \n",
    "        synthetic_small_X = self.find_categorical(\n",
    "            suitable_generated_small_X, categorical_small_X, small_X \n",
    "        )\n",
    "       \n",
    "        # small class와 large class 합치기\n",
    "        origin_small_x = pd.concat(\n",
    "            [midlle_small_X, categorical_small_X.loc[midlle_small_X.index]], axis=1\n",
    "        )\n",
    "   \n",
    "        small_total_x = pd.concat([synthetic_small_X, origin_small_x], axis=0)\n",
    "        small_total_x[\"target\"] = small_Y[0]\n",
    "\n",
    "        origin_large_x = pd.concat(\n",
    "            [midlle_large_X, categorical_large_X.loc[midlle_large_X.index]], axis=1\n",
    "        )\n",
    "\n",
    "        origin_large_x[\"target\"] = small_Y[0]\n",
    "        total = pd.concat([small_total_x, origin_large_x], axis=0)\n",
    "        return total.drop(columns=[\"target\"]), total[\"target\"]\n",
    "    \n",
    "    \n",
    "    def fit(\n",
    "        self,\n",
    "        small_X: pd.DataFrame,\n",
    "        small_Y: pd.DataFrame,\n",
    "        large_X: pd.DataFrame,\n",
    "        large_Y: pd.DataFrame        \n",
    "    ):\n",
    "        \"\"\"\n",
    "        데이터를 학습 시키는 함수\n",
    "        Args:\n",
    "            small_X (pd.DataFrame): small class의 x\n",
    "            small_Y (pd.DataFrame): small class의 y\n",
    "            large_X (pd.DataFrame): large class의 x\n",
    "            large_Y (pd.DataFrame): large class의 y\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.DataFrame]: synthetic X, y\n",
    "        \n",
    "        \"\"\"\n",
    "        # 합성+ 기존 data set 생성\n",
    "        synthetic_X, synthetic_Y = self.generate_synthetic(\n",
    "            small_X, large_X, small_Y, large_Y\n",
    "        )\n",
    "        return synthetic_X, synthetic_Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEN = FiGen(0.3,['amenities_fee','room_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = has_rewards\n",
    "real_data = real_data.dropna(axis=0)\n",
    "small_X = real_data[real_data['has_rewards'] == True]\n",
    "small_Y = real_data[real_data['has_rewards'] == True].iloc[:, [1]]\n",
    "large_X = real_data[real_data['has_rewards'] == False]\n",
    "large_Y = real_data[real_data['has_rewards'] == False].iloc[:, [1]]\n",
    "\n",
    "# 생성된 것 중에 적합헌 것이 없어 넘어가는 과장이 원활히 되지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_X, synthetic_Y, test = GEN.fit(small_X, small_Y, large_X, large_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_index = ['amenities_fee','room_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "continue_small_X = small_X[con_index]\n",
    "categorical_small_X = small_X[list(set(small_X.columns) - set(con_index))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_categorical(\n",
    "        suitable_generated_small_X: pd.DataFrame, categorical_small_X: pd.DataFrame, small_X: pd.DataFrame\n",
    "    ):  \n",
    "        \"\"\"\n",
    "        생성된 연속형변수와 기존 연속형 변수의 cosine simmilarity를 기준으로 가장 가까운 기존 변수를 찾은 후 해당 변수의 범주형 값을 가져옴\n",
    "        \n",
    "        Args:\n",
    "            suitable_generated_small_X : 생성된 적합한 small class의 연속형 변수만 있는 x \n",
    "            small_X : small class의 연속형, 범주형 변수가 모두 있는 orgin x\n",
    "        Returns:\n",
    "            생성된 연속 변수를 범주형 변수값이 결합된 형태로 리턴 \n",
    "        \"\"\"\n",
    "\n",
    "        # Min-Max 스케일링을 위한 객체 생성\n",
    "        scaler = MinMaxScaler()\n",
    "\n",
    "        # 열별 Min-Max 스케일링 수행\n",
    "        suitable_generated_small_scaled_X = pd.DataFrame(\n",
    "            scaler.fit_transform(suitable_generated_small_X),\n",
    "            columns=suitable_generated_small_X.columns,\n",
    "        )\n",
    " \n",
    "        orgin_small_non_cat_scaled_X = pd.DataFrame(\n",
    "            scaler.fit_transform(small_X[con_index]),\n",
    "            columns=con_index\n",
    "        )\n",
    "\n",
    "        # 데이터프레임을 numpy 배열로 변환\n",
    "        array_mxn = suitable_generated_small_scaled_X.values\n",
    "        array_kxn = orgin_small_non_cat_scaled_X.values\n",
    "        \n",
    "        # 행렬곱 수행 (mxn과 nxk로 계산)\n",
    "        result_array = np.dot(array_mxn, array_kxn.T)\n",
    "        # 각 행에서 최대값을 가지는 열의 인덱스를 가져와서 리스트로 만들기\n",
    "        max_indices = np.argmax(result_array, axis=1).tolist()\n",
    "\n",
    "        # 가장큰 열 인덱스가 들어있는 리스트의 인덱스에 따라 범주형 값 가져오기\n",
    "        synthetic_small_X = pd.concat(\n",
    "            [suitable_generated_small_scaled_X, categorical_small_X.loc[max_indices]],axis=1\n",
    "        )\n",
    "\n",
    "        return synthetic_small_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_small_X = find_categorical(\n",
    "            continue_small_X.iloc[:10], categorical_small_X, small_X )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continue_small_X.iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_np = [1.00000000e+00, 3.30228798e-01, 0.00000000e+00, 6.15400836e-02,\n",
    "  3.57431554e-02, 3.53530383e-01, 5.13478368e-02, 4.87646294e-01,\n",
    "  2.10065722e-01, 1.11236074e-01, 1.02273925e-01, 1.69402172e-02,\n",
    "  2.04336977e-01, 4.98682037e-01, 3.27030542e-01, 3.51456788e-02,\n",
    "  1.79594419e-02, 6.45661266e-01, 4.22661934e-01, 4.27617474e-01,\n",
    "  2.73538818e-01, 5.15165360e-01, 2.39904404e-01, 4.37141953e-01,\n",
    "  4.64801603e-01, 2.08976206e-01, 4.80722595e-01, 4.31413208e-01,\n",
    "  4.32362141e-01, 4.19955716e-01, 4.19955716e-01, 4.76926862e-02,\n",
    "  2.90654764e-02, 2.56704038e-01, 5.48342881e-01, 2.53013742e-01,\n",
    "  4.35384669e-01, 6.60738762e-03, 3.07348961e-01, 3.12796542e-03,\n",
    "  7.83397181e-02, 3.18068393e-01, 3.01303905e-01, 4.25122131e-01,\n",
    "  3.17927811e-01, 7.92640495e-01, 4.77664921e-01, 4.22837662e-01,\n",
    "  4.24067761e-01, 4.55734018e-01, 5.34249464e-01, 4.29761361e-01,\n",
    "  2.47495870e-01]\n",
    "\n",
    "max(test_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장큰 열 인덱스가 들어있는 리스트의 인덱스에 따라 범주형 값 가져오기 -> 값이 중복되는 경우에 대차 Ex ) 0,0,0,0, "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
