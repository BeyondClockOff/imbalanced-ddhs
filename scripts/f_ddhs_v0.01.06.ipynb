{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.datasets.demo import download_demo\n",
    "real_data, metadata = download_demo(\n",
    "    modality='single_table',\n",
    "    dataset_name='fake_hotel_guests')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.lite import SingleTablePreset\n",
    "\n",
    "synthesizer = SingleTablePreset(metadata, name='FAST_ML')\n",
    "synthesizer.fit(data=real_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from sdv.single_table import CTGANSynthesizer\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sdv.metadata import SingleTableMetadata\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sdv.lite import SingleTablePreset\n",
    "\n",
    "# push + prd\n",
    "\n",
    "# imbalanced에 data level로 해결하는 모델\n",
    "class FiGen:\n",
    "    def __init__(self, ratio: float, index: List[str]):\n",
    "        \"\"\"\n",
    "        고정적으로 사용하는 값을 저장\n",
    "        \n",
    "        Args:\n",
    "            ratio (float): small class+생성된 데이터와 large class의 비율 \n",
    "            index (List[int]): 범주형, 연속형 구분하기 위한 연속형 변수의 컬럼명 인덱스       \n",
    "        \"\"\"\n",
    "        self.result = 0\n",
    "        self.ratio = ratio\n",
    "        self.index = index\n",
    "\n",
    "\n",
    "    def extract_middle_percent(self, data: pd.DataFrame, start: float, last:float):\n",
    "        \"\"\"\n",
    "        데이터의 분포 중 중간 부분을 추출 \n",
    "        \n",
    "        Args:\n",
    "            data : 입력 데이터\n",
    "            start : 추출 시작 percentile \n",
    "            last : 추출 끝 percentile\n",
    "        Returns:    \n",
    "            데이터의 분포 중 중간 부분을 추출하여 리턴\n",
    "        \"\"\"\n",
    "        scaler = StandardScaler()\n",
    "        data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "        kde = KernelDensity(kernel=\"gaussian\", bandwidth=0.5).fit(\n",
    "            data_scaled\n",
    "        )  ##TODO: 계산이 안터지도록 하기, gmm으로 변경\n",
    "\n",
    "        log_prob = kde.score_samples(data_scaled)\n",
    "        prob = np.exp(log_prob)\n",
    "        threshold_low, threshold_high = np.percentile(prob, [start, last])\n",
    "        mask = np.logical_and(prob >= threshold_low, prob <= threshold_high)\n",
    "        data_middle = data[mask]\n",
    "\n",
    "        if len(data_middle) > 0:\n",
    "            return data_middle\n",
    "        else:\n",
    "            print(\"No middle 50% found, returning original data\")\n",
    "            return np.array([])\n",
    "\n",
    "    def find_categorical(\n",
    "        self, suitable_generated_small_X: pd.DataFrame, categorical_small_X: pd.DataFrame, small_X: pd.DataFrame\n",
    "    ):  # ****************\n",
    "        \"\"\"\n",
    "        생성된 연속형변수와 기존 연속형 변수의 cosine simmilarity를 기준으로 가장 가까운 기존 변수를 찾은 후 해당 변수의 범주형 값을 가져옴\n",
    "        \n",
    "        Args:\n",
    "            suitable_generated_small_X : 생성된 적합한 small class의 연속형 변수만 있는 x \n",
    "            small_X : small class의 연속형, 범주형 변수가 모두 있는 orgin x\n",
    "        Returns:\n",
    "            생성된 연속 변수를 범주형 변수값이 결합된 형태로 리턴 \n",
    "        \"\"\"\n",
    "\n",
    "        # Min-Max 스케일링을 위한 객체 생성\n",
    "        scaler = MinMaxScaler()\n",
    "\n",
    "        # 열별 Min-Max 스케일링 수행\n",
    "        suitable_generated_small_scaled_X = pd.DataFrame(\n",
    "            scaler.fit_transform(suitable_generated_small_X),\n",
    "            columns=suitable_generated_small_X.columns,\n",
    "        )\n",
    "\n",
    "        orgin_small_non_cat_scaled_X = pd.DataFrame(\n",
    "            scaler.fit_transform(small_X.iloc[:, self.index]),\n",
    "            columns=small_X.iloc[:, self.index].columns,\n",
    "        )\n",
    "\n",
    "        # 데이터프레임을 numpy 배열로 변환\n",
    "        array_mxn = suitable_generated_small_scaled_X.values\n",
    "        array_kxn = orgin_small_non_cat_scaled_X.values\n",
    "\n",
    "        # 행렬곱 수행 (mxn과 nxk로 계산)\n",
    "        result_array = np.dot(array_mxn, array_kxn)\n",
    "\n",
    "        # 각 행에서 최대값을 가지는 열의 인덱스를 가져와서 리스트로 만들기\n",
    "        max_indices = np.argmax(result_array, axis=1).tolist()\n",
    "\n",
    "        # 가장큰 열 인덱스가 들어있는 리스트의 인덱스에 따라 범주형 값 가져오기\n",
    "        synthetic_small_X = pd.concat(\n",
    "            [suitable_generated_small_scaled_X, categorical_small_X.loc[max_indices]],\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "        return synthetic_small_X\n",
    "\n",
    "    def suitable_judge(self, midlle_small_X:pd.DataFrame, small_X: pd.DataFrame, large_X: pd.DataFrame):\n",
    "        \"\"\"\n",
    "           generated_x : 생성된 small class x 데이터\n",
    "           small_X : 원본 small class x 데이터\n",
    "           large_X : 원본 large class x 데이터\n",
    "        \"\"\"\n",
    "\n",
    "        center_small_X = np.mean(\n",
    "            small_X.cpu().numpy(), axis=1, dtype=np.float64, out=None \n",
    "        )\n",
    "\n",
    "        radius_small_X = np.max(\n",
    "            np.linalg.norm(small_X.cpu().numpy() - center_small_X, axis=1)\n",
    "        )\n",
    "\n",
    "        center_large_X = np.mean(\n",
    "            large_X.cpu().numpy(), axis=1, dtype=np.float64, out=None \n",
    "        )\n",
    "\n",
    "        radius_large_X = np.max(\n",
    "            np.linalg.norm(large_X.cpu().numpy() - center_large_X, axis=1)\n",
    "        )\n",
    "\n",
    "        synthetic_sample = pd.DataFrame()  # 최종 합치기\n",
    "        \n",
    "\n",
    "        # ctgan으로 연속형 생성 부분\n",
    "        metadata = SingleTableMetadata()\n",
    "        metadata.detect_from_dataframe(data=midlle_small_X)\n",
    "        \n",
    "        synthesizer = SingleTablePreset(metadata, name='FAST_ML')\n",
    "        synthesizer.fit(data=midlle_small_X)\n",
    "                \n",
    "            \n",
    "        # large class의 데이터 사이즈 만큼 데이터 생성\n",
    "        synthetic_data = synthesizer.sample(num_rows=len(large_X)) # 새로운 데이터를 생성하는 비용 vs 데이터가 적합한지 판단하는 비용 \n",
    "                \n",
    "\n",
    "        # 합성된 개수 / 원래 클래스 개수 <= ratio 만족시 그만 생성하는 것으로\n",
    "        i = 0 \n",
    "        while len(synthetic_sample) / len(large_X) >= self.ratio:\n",
    "            \n",
    "            z = synthetic_data.loc[i]\n",
    "\n",
    "            # 생성된 small class 데이터가 small, large class 중 small에 가까운지, small class의 지름을 넘지는 않는지\n",
    "            if (\n",
    "                np.linalg.norm(z - center_small_X) < np.linalg.norm(z - center_large_X)\n",
    "                and np.linalg.norm(z - center_small_X) < radius_small_X\n",
    "            ):\n",
    "                synthetic_sample.append(z)  \n",
    "                \n",
    "            i+=1\n",
    "            \n",
    "            # 생성된 샘플을 다 검정해도 생성 비율을 만족하지 못할 경우\n",
    "            if i+1 == len(large_X) and len(synthetic_sample) / len(large_X) < self.ratio:\n",
    "                i=0\n",
    "                synthetic_data = synthesizer.sample(num_rows=len(large_X))\n",
    "                \n",
    "        return synthetic_sample.reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    def generate_synthetic(\n",
    "        self, small_X: pd.DataFrame, large_X: pd.DataFrame, small_Y: pd.DataFrame, large_Y: pd.DataFrame\n",
    "    ) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "        \"\"\"\n",
    "        생성된 데이터셋 + 기존 데이터셋을 합쳐 통합 데이터셋을 생성\n",
    "        \n",
    "        Args:\n",
    "            small_X (pd.DataFrame): small class의 x\n",
    "            large_X (pd.DataFrame): large class의 x\n",
    "        Returns:\n",
    "            생성된 데이터셋 + 기존 데이터셋을 합쳐 통합 데이터셋을 리턴\n",
    "        \"\"\"\n",
    "        # 연속형 변수만 가져오는 부분\n",
    "        continue_small_X = small_X.iloc[:, self.index]\n",
    "        continue_large_X = large_X.iloc[:, self.index]\n",
    "\n",
    "        # 범주형 변수만 가져오는 부분\n",
    "        categorical_small_X = small_X.iloc[\n",
    "            :, [i for i in range(len(small_X.columns)) if i not in self.index]\n",
    "        ]\n",
    "        categorical_large_X = large_X.iloc[\n",
    "            :, [i for i in range(len(small_X.columns)) if i not in self.index]\n",
    "        ]\n",
    "\n",
    "        # 상위 n% 필터링 부분\n",
    "        midlle_small_X = self.extract_middle_percent(\n",
    "            continue_small_X, 25, 75\n",
    "        )  ##TODO: 추후에 하이퍼 파라미터로 뺄 수 있음\n",
    "\n",
    "        midlle_large_X = self.extract_middle_percent(\n",
    "            continue_large_X, 15, 85\n",
    "        )  ##TODO: 추후에 하이퍼 파라미터로 뺄 수 있음\n",
    "\n",
    "        # 연속형 데이터 생성 및 데이터 적합 판단\n",
    "\n",
    "        suitable_generated_small_X = self.suitable_judge(midlle_small_X, small_X, large_X)\n",
    "\n",
    "        # 코사인 유사도 기반으로 가장 가까운 기존 변수의 범주형 변수 값 가져오기\n",
    "\n",
    "        synthetic_small_X = self.find_categorical(\n",
    "            suitable_generated_small_X, categorical_small_X, small_X ,self.index\n",
    "        )\n",
    "\n",
    "        # small class와 large class 합치기\n",
    "\n",
    "        origin_small_x = pd.concat(\n",
    "            [midlle_small_X, categorical_small_X.loc[midlle_small_X.index]], axis=1\n",
    "        )\n",
    "\n",
    "        small_total_x = pd.concat([synthetic_small_X, origin_small_x], axis=0)\n",
    "\n",
    "        small_total_x[\"target\"] = small_Y[0]\n",
    "\n",
    "        origin_large_x = pd.concat(\n",
    "            [midlle_large_X, categorical_large_X.loc[midlle_large_X.index]], axis=1\n",
    "        )\n",
    "\n",
    "        origin_large_x[\"target\"] = small_Y[0]\n",
    "        total = pd.concat([small_total_x, origin_large_x], axis=0)\n",
    "        return total.drop(columns=[\"target\"]), total[\"target\"]\n",
    "    \n",
    "    \n",
    "    def fit(\n",
    "        self,\n",
    "        small_X: pd.DataFrame,\n",
    "        small_Y: pd.DataFrame,\n",
    "        large_X: pd.DataFrame,\n",
    "        large_Y: pd.DataFrame,\n",
    "        ratio : float ,\n",
    "        index : list[int]\n",
    "        \n",
    "    ):\n",
    "        \"\"\"\n",
    "        데이터를 학습 시키는 함수\n",
    "        Args:\n",
    "            small_X (pd.DataFrame): small class의 x\n",
    "            small_Y (pd.DataFrame): small class의 y\n",
    "            large_X (pd.DataFrame): large class의 x\n",
    "            large_Y (pd.DataFrame): large class의 y\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.DataFrame]: synthetic X, y\n",
    "        \n",
    "        \"\"\"\n",
    "        # 합성+ 기존 data set 생성\n",
    "        synthetic_X, synthetic_Y = self.generate_synthetic(\n",
    "            small_X, large_X,small_Y, large_Y\n",
    "        )\n",
    "        return synthetic_X, synthetic_Y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEN = FiGen(0.3, [2,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = has_rewards\n",
    "small_X = real_data[real_data['has_rewards'] == True]\n",
    "small_Y = real_data[real_data['has_rewards'] == True].iloc[:, [1]]\n",
    "large_X = real_data[real_data['has_rewards'] == False]\n",
    "large_Y = real_data[real_data['has_rewards'] == False].iloc[:, [1]]\n",
    "ratio = 0.3\n",
    "index = [2,5]\n",
    "GEN.fit(small_X, small_Y, large_X, large_Y, 0.3, [2,5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_X.iloc[:, [i for i in range(len(small_X.columns)) if i not in index] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
