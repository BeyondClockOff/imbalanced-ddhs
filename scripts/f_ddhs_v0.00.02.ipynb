{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c182b8be",
   "metadata": {},
   "source": [
    "1. 연속형, 범주형 판단\n",
    "2. small, big class에 따라 분리\n",
    "3. 각 class에 따로 연속형만 GMM(k=2) fitting 후 상위 N % 만 Filtering\n",
    "4. ctgan으로 연속형 변수만 생성 후 Density 기반의 적합성 판단\n",
    "5. 생성된 연속형 변수와 cosine similarity 기반의 가장 가까운 기존 변수의 범주형값 카피"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8988d80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ctgan\n",
    "!pip install sdv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad06f265",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.single_table import CTGANSynthesizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d751408",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.datasets.demo import download_demo\n",
    "\n",
    "data, metadata = download_demo(\n",
    "    modality='single_table',\n",
    "    dataset_name='fake_hotel_guests'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff43cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "synthesizer = CTGANSynthesizer(metadata)\n",
    "synthesizer.fit(data)\n",
    "\n",
    "synthetic_data = synthesizer.sample(num_rows=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf30133",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01aa625a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GMM: n_components = 모델의 총 수\n",
    "gmm = GaussianMixture(n_components=3, random_state=0)\n",
    "gmm.fit(iris.data)\n",
    "gmm_cluster_labels = gmm.predict(iris.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b8358f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cf17b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.tabular import CTGAN\n",
    "from sdv.evaluation import evaluate\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "batch_size = 5000\n",
    "epochs = 100\n",
    "model = CTGAN(batch_size=batch_size, epochs=epochs, verbose=True)\n",
    "model.fit(train_df)\n",
    "model.save('ctgan_30daysofml_model.pkl')\n",
    "\n",
    "n_generated_data = 1000\n",
    "generated_df = model.sample(n_generated_data)\n",
    "\n",
    "score = evaluate(generated_df, train_df.sample(n_generated_data))\n",
    "score\n",
    "\n",
    "# ctgan\n",
    "class Autoencoder(nn.Module):\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "# imbalanced에 data level로 해결하는 모델\n",
    "class DDHS:\n",
    "    \n",
    "  #데이터를 KDE로 가우시안 분포를 이용하여 중간 %를 추출하는 함수\n",
    "  # start, last에 퍼센트를 입력\n",
    "\n",
    "  def extract_middle_percent(self,data, start, last):\n",
    "    scaler = StandardScaler()\n",
    "    data_scaled = scaler.fit_transform(data)\n",
    "    kde = KernelDensity(kernel='gaussian', bandwidth=0.5).fit(data_scaled)\n",
    "    \n",
    "    log_prob = kde.score_samples(data_scaled)\n",
    "    prob = np.exp(log_prob)\n",
    "    threshold_low, threshold_high = np.percentile(prob, [start, last])\n",
    "    mask = np.logical_and(prob >= threshold_low, prob <= threshold_high) #######\n",
    "    data_middle = data[mask]\n",
    "\n",
    "    if len(data_middle) > 0 :\n",
    "      return data_middle\n",
    "    else:  \n",
    "      print(\"No middle 50% found, returning original data\")\n",
    "      return np.array([])\n",
    "\n",
    "  #  각 feature 안의 값을 복원추출하는 함수\n",
    "\n",
    "  def reconstruct_features(self,data):\n",
    "    mean = data.mean(axis=0)\n",
    "    std = data.std(axis=0)\n",
    "    reconstructed = np.random.randn(*data.shape) * std + mean\n",
    "    return reconstructed\n",
    "\n",
    "  # synthetic sample을 생성하는 함수\n",
    "  # 라벨과과 데이터를 하나의 데이터프레임으로 출력\n",
    "  # small class / large class의 비율 = ratio \n",
    "\n",
    "\n",
    "  # 디폴트트 파라미터는 뒤로 빼서 입력하기\n",
    "  def generate_synthetic_sample(self,X,Y, ratio=0.3):\n",
    "\n",
    "    data = pd.concat([X,Y],axis=1)\n",
    "\n",
    "    # small class\n",
    "    data_A = data[data[Y.columns[0]]== Y.value_counts().idxmin()[0]  ].loc[:, data.columns != Y.columns[0]].astype(float).values\n",
    "\n",
    "    # large class \n",
    "    data_B = data[data[Y.columns[0]]== Y.value_counts().idxmax()[0] ].loc[:, data.columns != Y.columns[0]].astype(float).values\n",
    "\n",
    "    # autoencoder를 사용하여 잠재 변수를 추출\n",
    "    with torch.no_grad():\n",
    "        encoded_A, _ = self.model(torch.tensor(data_A).float())\n",
    "        encoded_B, _ = self.model(torch.tensor(data_B).float())\n",
    "  \n",
    "    # Majority : 입력받은 퍼센트 보다 Density가 큰 샘플을 Keep→ 입력받은 퍼센트 샘플을 Classifier 의 Train 데이터로 활용\n",
    "    # Minority : 입력받은 퍼센트 을 사용해서 더 높은 기준을 설정 → 입력받은 퍼센트 샘플을 Classifier 의 Train 데이터로 활용 → 25% 샘플을 Subsequence 생성 과정의 샘플로 활용\n",
    "  \n",
    "    encoded_A_middle = self.extract_middle_percent(encoded_A.cpu().numpy(),50 - self.small_percent/2 , 50+ self.small_percent/2) \n",
    "    encoded_B_middle = self.extract_middle_percent(encoded_B.cpu().numpy(),50 -self.large_percent/2, 50 + self.large_percent/2)\n",
    "\n",
    "    # 중간 25%의 잠재 변수로부터 feature를 복원추출 \n",
    "    reconstructed_features = self.reconstruct_features(self.extract_middle_percent(encoded_A.cpu().numpy(),37.5, 62.5))\n",
    "    # 임의의 위치에 synthetic sample 생성\n",
    "    center_A = np.mean(encoded_A.cpu().numpy(), axis=0, dtype=np.float64, out=None)\n",
    "\n",
    "    center_B = np.mean(encoded_B.numpy(), axis=0, dtype=np.float64, out=None) \n",
    "\n",
    "    radius_A = np.max(np.linalg.norm(encoded_A.cpu().numpy() - center_A, axis=1))\n",
    "\n",
    "    synthetic_sample = pd.DataFrame() # 최종 합치기\n",
    "   \n",
    "   # 합성된 개수 / 원래 클래스 개수\n",
    "    while len(synthetic_sample)/len(data_A) >= ratio :\n",
    "        z = np.random.randn(latent_dim)\n",
    "        if np.linalg.norm(z - center_A) < np.linalg.norm(z - center_B) and np.linalg.norm(z - center_A) < radius_A:\n",
    "            synthetic_sample.append(z) #, ignore_index=True)\n",
    "\n",
    "    # 최종 출력할 데이터 \n",
    "    encoded_B_middle = pd.DataFrame(encoded_B_middle)\n",
    "    encoded_B_middle['label'] = Y.value_counts().idxmin()[0]\n",
    "\n",
    "    encoded_A_middle = pd.DataFrame(encoded_A_middle)\n",
    "    encoded_A_middle['label'] = Y.value_counts().idxmax()[0] \n",
    "\n",
    "    synthetic_sample['label'] = Y.value_counts().idxmax()[0] \n",
    "\n",
    "    ouput = pd.concat([encoded_B_middle,encoded_A_middle,synthetic_sample ] )\n",
    "\n",
    "    x_ = ouput.loc[:, ouput.columns != 'label']\n",
    "    x_.columns = X.columns\n",
    "    y_ = ouput['label']\n",
    "    y_.columns = Y.columns\n",
    "\n",
    "    return x_ , y_\n",
    "\n",
    "  def fit(self,X,Y,large_percent = 50 , small_percent = 75 ,lr = 1e-3 ,num_epochs = 50, ratio = 1):\n",
    "    self.large_percent = large_percent\n",
    "    self.small_percent = small_percent\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    input_dim = len(X.columns) # 데이터의 차원\n",
    "    latent_dim = len(X.columns) # 잠재 변수의 차원\n",
    "    #ratio =  합성된 데이터 수/small class  비율\n",
    "    #large_percent : large class의 추출 비율\n",
    "    #small_percent : small class의 추출 비율 \n",
    "    #lr = 1e-3 # 학습률\n",
    "    #num_epochs = 50 # 학습 에폭 수\n",
    "    \n",
    "    self.model = Autoencoder(input_dim, latent_dim).to(device)\n",
    "    optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "      x = torch.tensor(X.to_numpy()).float()#.to(device)\n",
    "      y = torch.tensor(Y[Y.columns[0]].to_numpy()).float().to(device)\n",
    "\n",
    "      encoded, decoded = self.model(x)\n",
    "      reconstruction_loss = F.mse_loss(decoded, x)\n",
    "      center_loss = self.model.get_center_loss(encoded, y)\n",
    "\n",
    "      loss = reconstruction_loss + center_loss\n",
    "      cross_entropy_loss = F.cross_entropy(decoded, y.long()) # y를 long 형으로 요구\n",
    "      loss += cross_entropy_loss\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "    \n",
    "    synthetic_X, synthetic_Y = self.generate_synthetic_sample(X,Y, ratio)\n",
    "    return synthetic_X, synthetic_Y\n",
    "\n",
    "\n",
    "  def __init__(self):\n",
    "    self.result = 0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45963b8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b029c9f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b273bc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad54eda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df467dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "magic = pd.read_csv('/Users/jangsehwan/Documents/DDHS개발/magic04.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb85710",
   "metadata": {},
   "outputs": [],
   "source": [
    "magic.iloc[:, [1,4,6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bef12aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 순차적으로 구조 점검 및 구현 \n",
    "# 범주형 변수 찾아내는 부분 추가 \n",
    "# ctgan, gmm 버전 오류해결 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca3174b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.single_table import CTGANSynthesizer\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "# imbalanced에 data level로 해결하는 모델\n",
    "class figen:\n",
    "    def fit(self,small_X,small_Y,large_X,large_Y,ratio,index):\n",
    "        '''\n",
    "            index : 범주형, 연속형 구분 인덱스\n",
    "            small_X : small class의 x\n",
    "            small_Y : small class의 y\n",
    "            large_X : large class의 x\n",
    "            large_Y : large class의 y\n",
    "            ratio : small class+생성된 데이터와 large class의 비율 \n",
    "            \n",
    "            \n",
    "        '''\n",
    "        self.index = index\n",
    "        self.ratio = ratio\n",
    "        self.index = index \n",
    "    \n",
    "        # 합성+ 기존 data set 생성 \n",
    "        synthetic_X, synthetic_Y = self.generate_synthetic(small_X,large_Y,self.ratio, self.index)\n",
    "        \n",
    "        \n",
    "        return synthetic_X, synthetic_Y\n",
    "    \n",
    "    \n",
    "    def generate_synthetic(self, small_X, large_X, ratio, index ):\n",
    "        '''\n",
    "            small_X : \n",
    "            large_X : \n",
    "            ratio : small class와 large class 의 비율 \n",
    "            index : 연속형 변수의 컬럼 인덱스\n",
    "        '''\n",
    "        \n",
    "        # 연속형 변수만 가져오는 부분 \n",
    "        continue_small_X = small_X.iloc[:,self.index]\n",
    "        continue_large_X = large_X.iloc[:,self.index]\n",
    "        \n",
    "        \n",
    "        # 상위 n% 필터링 부분 \n",
    "        midlle_small_X = extract_middle_percent(self,continue_small_X, 25, 75) # 추후에 하이퍼 파라미터로 뺄 수 있음\n",
    "        \n",
    "        midlle_large_X = extract_middle_percent(self,continue_large_X, 15, 85) # 추후에 하이퍼 파라미터로 뺄 수 있음\n",
    "                \n",
    "        \n",
    "        # ctgan으로 연속형 생성 부분 \n",
    "        \n",
    "        synthesizer.fit(midlle_small_X)\n",
    "        \n",
    "        synthetic_data = synthesizer.sample(num_rows=10) \n",
    "        \n",
    "        # 데이터 적합 판단 \n",
    "        \n",
    "        suitable_generated_small_X = suitable_judge(synthetic_data)\n",
    "        \n",
    "        # 범주형 변수 값 찾아내기\n",
    "        \n",
    "        find_categorical()\n",
    "        \n",
    "        \n",
    "        # small class와 large class 합치기\n",
    "        \n",
    "        return synthetic_X, synthetic_Y\n",
    "        \n",
    "\n",
    "    def extract_middle_percent(self,data, start, last):\n",
    "        '''\n",
    "            data : 입력 데이터\n",
    "            start : 추출 시작 percentile \n",
    "            last : 추출 끝 percentile\n",
    "        \n",
    "            \n",
    "        '''\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        data_scaled = scaler.fit_transform(data)\n",
    "        \n",
    "        kde = KernelDensity(kernel='gaussian', bandwidth=0.5).fit(data_scaled) # 계산이 안터지도록 하기, gmm으로 변경 \n",
    "    \n",
    "        log_prob = kde.score_samples(data_scaled)\n",
    "        prob = np.exp(log_prob)\n",
    "        threshold_low, threshold_high = np.percentile(prob, [start, last])\n",
    "        mask = np.logical_and(prob >= threshold_low, prob <= threshold_high) \n",
    "        data_middle = data[mask]\n",
    "\n",
    "        if len(data_middle) > 0 :\n",
    "            return data_middle\n",
    "        else:  \n",
    "            print(\"No middle 50% found, returning original data\")\n",
    "            return np.array([])\n",
    "\n",
    "        \n",
    "        \n",
    "    def find_categorical():\n",
    "        '''\n",
    "        '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def suitable_judge(generated_x, small_X, large_X):\n",
    "        '''\n",
    "           generated_x : 생성된 small class x 데이터\n",
    "           small_X : 원본 small class x 데이터\n",
    "           large_X : 원본 large class x 데이터\n",
    "        '''\n",
    "        \n",
    "        center_small_X = np.mean(small_X.cpu().numpy(), axis=0, dtype=np.float64, out=None)\n",
    "        \n",
    "        radius_small_X = np.max(np.linalg.norm(small_X.cpu().numpy() - center_small_X, axis=1))\n",
    "        \n",
    "        center_large_X = np.mean(large_X.cpu().numpy(), axis=0, dtype=np.float64, out=None)\n",
    "        \n",
    "        radius_large_X = np.max(np.linalg.norm(large_X.cpu().numpy() - center_large_X, axis=1))\n",
    "\n",
    "        synthetic_sample = pd.DataFrame() # 최종 합치기\n",
    "        \n",
    "        # 합성된 개수 / 원래 클래스 개수 <= ratio\n",
    "        while len(synthetic_sample)/len(large_X) >= ratio :\n",
    "            \n",
    "            z = generated_x # 얼마나 생성하는 부분 추가, 조건문으로 생성을 N개 후 len(synthetic_sample)/len(large_X) >= ratio 만족시 그만 생성하는 것으로\n",
    "            \n",
    "            # 생성된 small class 데이터가 small, large class 중 small에 가까운지, small class의 지름을 넘지는 않는지\n",
    "            if np.linalg.norm(z - center_small_X) < np.linalg.norm(z - center_large_X ) and np.linalg.norm(z - center_small_X) < radius_small_X:\n",
    "                synthetic_sample.append(z) #, ignore_index=True)\n",
    "                \n",
    "        return synthetic_sample\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        self.result = 0\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44015f82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
